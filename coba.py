from flask import Flask, render_template, request
import numpy as np
import pandas as pd
import nltk
import pickle
import joblib
import json
import random
import string

from nltk.stem import WordNetLemmatizer
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

app = Flask(__name__)

# with open('Other/waste_management3.json', encoding='utf-8') as content:
#   data = json.load(content)

# # Package sentence tokenizer
# nltk.download('punkt')
# # Package lemmatization
# nltk.download('wordnet')
# # Package multilingual wordnet data
# nltk.download('omw-1.4')

# # Mendapatkan semua data ke dalam list
# tags = [] # data tag
# inputs = [] # data input atau pattern
# responses = {} # data respon
# words = [] # Data kata
# classes = [] # Data Kelas atau Tag
# documents = [] # Data Kalimat Dokumen
# ignore_words = ['?', '!'] # Mengabaikan tanda spesial karakter

# for intent in data['intents']:
#   responses[intent['tag']]=intent['responses']
#   for lines in intent['patterns']:
#     inputs.append(lines)
#     tags.append(intent['tag'])
#     for pattern in intent['patterns']:
#       w = nltk.word_tokenize(pattern)
#       words.extend(w)
#       documents.append((w, intent['tag']))
#       # add to our classes list
#       if intent['tag'] not in classes:
#         classes.append(intent['tag'])

# pickle.dump(responses,open('responses.pkl','wb'))

# # Konversi data json ke dalam dataframe
# # data = pd.DataFrame({"patterns":inputs, "tags":tags})

# # Split dataset into training set and test set
# X_train, X_test, y_train, y_test = train_test_split(inputs, tags,test_size=0.2, random_state=42)

# # Konversi data ke dalam dataframe
# train_data = pd.DataFrame({"patterns": X_train, "tags": y_train})
# test_data = pd.DataFrame({"patterns": X_test, "tags": y_test})

# train_data['patterns'] = train_data['patterns'].apply(lambda wrd: [ltrs.lower() for ltrs in wrd if ltrs not in string.punctuation])
# train_data['patterns'] = train_data['patterns'].apply(lambda wrd: ''.join(wrd))
# test_data['patterns'] = test_data['patterns'].apply(lambda wrd: [ltrs.lower() for ltrs in wrd if ltrs not in string.punctuation])
# test_data['patterns'] = test_data['patterns'].apply(lambda wrd: ''.join(wrd))

# # Inisialisasi Lemmatizer
# lemmatizer = WordNetLemmatizer()

# # Fungsi untuk melakukan lemmatization pada kata-kata dalam kalimat
# def lemmatize_text(text):
#     tokens = nltk.word_tokenize(text)  # Tokenisasi kata-kata dalam kalimat
#     lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]  # Lemmatization
#     lemmatized_text = ' '.join(lemmatized_tokens)  # Menggabungkan kembali kata-kata menjadi kalimat
#     return lemmatized_text

# # Contoh penggunaan fungsi lemmatize_text pada dataset
# train_data['patterns'] = train_data['patterns'].apply(lemmatize_text)
# test_data['patterns'] = test_data['patterns'].apply(lemmatize_text)

# # Tokenize the data (Tokenisasi Data)
# tokenizer = Tokenizer(num_words=2000)
# tokenizer.fit_on_texts(train_data['patterns'])

# x_train = tokenizer.texts_to_sequences(train_data['patterns'])
# x_test = tokenizer.texts_to_sequences(test_data['patterns'])

# pickle.dump(tokenizer,open('tokenizer.pkl','wb'))

# # Apply padding
# max_sequence_length = max(len(seq) for seq in x_train + x_test)
# x_train = pad_sequences(x_train, maxlen=max_sequence_length)
# x_test = pad_sequences(x_test, maxlen=max_sequence_length)

# pickle.dump(max_sequence_length,open('max_sequence_length.pkl','wb'))

# # Combine tags from train_data and test_data
# all_tags = list(set(train_data['tags']) | set(test_data['tags']))

# # Encoding the outputs
# le = LabelEncoder()
# le.fit(all_tags)

# y_train = le.transform(train_data['tags'])
# y_test = le.transform(test_data['tags'])

# pickle.dump(le,open('le.pkl','wb'))

# # Splitting the dataset
# train_dataset = (x_train, y_train)
# test_dataset = (x_test, y_test)

# Define your model and tokenizer here
from keras.models import load_model
chatbot_model = load_model('model.h5')
tokenizer = pickle.load(open('tokenizer.pkl','rb'))
max_sequence_length = pickle.load(open('max_sequence_length.pkl','rb'))
le = pickle.load(open('le.pkl','rb'))
responses = pickle.load(open('responses.pkl','rb'))

@app.route('/')
def home():
    return "hey"

@app.route('/chat', methods=['POST'])
def chat():
    texts_p = []
    prediction_input = request.form['message']

    # Remove punctuation and convert to lowercase
    prediction_input = [letter.lower() for letter in prediction_input if letter not in string.punctuation]
    prediction_input = ''.join(prediction_input)
    texts_p.append(prediction_input)

    # Tokenize and pad the input
    prediction_input = tokenizer.texts_to_sequences(texts_p)
    prediction_input = np.array(prediction_input).reshape(-1)
    prediction_input = pad_sequences([prediction_input], max_sequence_length)

    # Get the model's output prediction
    output = chatbot_model.predict(prediction_input)
    output = output.argmax()

    # Find the corresponding response based on the predicted tag
    response_tag = le.inverse_transform([output])[0]
    response = random.choice(responses[response_tag])

    return {'response': response}

if __name__ == '__main__':
    app.run()